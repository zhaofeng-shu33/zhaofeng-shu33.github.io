# Deep Learning Reproducibility
2019/10/20

We propose a Cloud DevOps-based method for this. GitLab large file storage is used to save the model (10GB per repo). Kaggle is used to save the data (20GB per dataset). To reduce the storage, we only save the latest version of the model and data. Also, model and data should be compressed before uploading to the server. We do not version the data and model since it will make the storage exploding, especially for model since it is frequently updated. Also, model cannot be saved as training artifact. Also, we think it is not necessary to make version of the data and model. Traditional computer algorithm only cares about code itself and often uses randomly gnerated data to test the algorithm (also some very small-scaled hand made data is used). In deep learning, method is highly coupled with data. In DevOps, we update the code frequently and old code has good results on old data. From this point of view, we should make version of the data. But the reality does not allow us to do so. This is one aspect. Another aspect is that dataset is the output of an output of another project. For most research, it is stable enough. We do not need to version it. Model is the output of training stage and one of input to the test stage. Indeed there are some drawbacks of only providing the latest version of model. Old version of test code may not produce good results. But we believe the verification of latest stable version is more important than hesitating on how to start.